{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW_j7nZOXZr9"
   },
   "source": [
    "# Assignment 2 - Task 2: Model Tuning\n",
    "\n",
    "This notebook tunes hyperparameters for two models using Keras Tuner and the existing processed splits:\n",
    "- Deep Neural Network (tabular)\n",
    "- GRU sequence model\n",
    "\n",
    "We search on the train/validation sets and keep the test set untouched for final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29453,
     "status": "ok",
     "timestamp": 1759231664052,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "sKDqqusCXZr_",
    "outputId": "71b6aaf4-c92e-47d4-b58f-1520d03f777c"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Try to import Keras Tuner; fall back to None\n",
    "try:\n",
    "    import keras_tuner as kt  # TF >=2.3\n",
    "except Exception:\n",
    "    try:\n",
    "        import kerastuner as kt  # legacy\n",
    "    except Exception:\n",
    "        kt = None\n",
    "        print('Keras Tuner not available; will use simple random search instead.')\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Paths\n",
    "from google.colab import drive\n",
    "drive.mount('/drive')\n",
    "DATA_DIR = Path('/drive/My Drive/Colab Notebooks/notebooks/data/processed')\n",
    "MODELS_DIR = Path('/drive/My Drive/Colab Notebooks/notebooks/models/tuned')\n",
    "ARTIFACTS_DIR = Path('/drive/My Drive/Colab Notebooks/notebooks/artifacts')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5346,
     "status": "ok",
     "timestamp": 1759231769464,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "1FPPUrfqXZsA",
    "outputId": "2bca20c0-ba9b-4c18-9dda-55ec051275d6"
   },
   "outputs": [],
   "source": [
    "# Ensure keras-tuner is available in this kernel\n",
    "if 'kt' in globals() and kt is None:\n",
    "    import sys, subprocess\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'keras-tuner'])\n",
    "        import keras_tuner as kt  # retry\n",
    "        print('Installed keras-tuner into current kernel.')\n",
    "    except Exception as e:\n",
    "        print('Failed to install keras-tuner:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1600,
     "status": "ok",
     "timestamp": 1759231776458,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "PHn5trSNXZsB",
    "outputId": "ed39e1a1-b5d0-4797-ea64-1796814e67bd"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv', parse_dates=['Month'])\n",
    "val_df = pd.read_csv(DATA_DIR / 'val.csv', parse_dates=['Month'])\n",
    "test_df = pd.read_csv(DATA_DIR / 'test.csv', parse_dates=['Month'])\n",
    "\n",
    "print('Shapes -> train:', train_df.shape, 'val:', val_df.shape, 'test:', test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1759231798881,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "rb0mwXwqXZsB",
    "outputId": "b618e617-9c72-4034-b9ae-94e2830955ff"
   },
   "outputs": [],
   "source": [
    "# Shared feature engineering\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['MonthSin'] = np.sin(2 * np.pi * df['MonthNum'] / 12.0)\n",
    "    df['MonthCos'] = np.cos(2 * np.pi * df['MonthNum'] / 12.0)\n",
    "    return df\n",
    "\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    add_time_features(frame)\n",
    "\n",
    "all_series_ids = pd.concat([\n",
    "    train_df['Series ID'], val_df['Series ID'], test_df['Series ID']\n",
    "], axis=0).unique()\n",
    "series_ids = sorted(all_series_ids)\n",
    "series_index = {sid: idx for idx, sid in enumerate(series_ids)}\n",
    "max_den = max(len(series_ids) - 1, 1)\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    frame['SeriesIndex'] = frame['Series ID'].map(series_index).astype('float32')\n",
    "    frame['SeriesIndexNorm'] = frame['SeriesIndex'] / max_den\n",
    "\n",
    "# Scalers for sequence features\n",
    "turnover_scaler = StandardScaler()\n",
    "year_scaler = StandardScaler()\n",
    "\n",
    "train_df['TurnoverScaled'] = turnover_scaler.fit_transform(train_df[['Turnover']])\n",
    "val_df['TurnoverScaled'] = turnover_scaler.transform(val_df[['Turnover']])\n",
    "test_df['TurnoverScaled'] = turnover_scaler.transform(test_df[['Turnover']])\n",
    "\n",
    "train_df['YearScaled'] = year_scaler.fit_transform(train_df[['Year']])\n",
    "val_df['YearScaled'] = year_scaler.transform(val_df[['Year']])\n",
    "test_df['YearScaled'] = year_scaler.transform(test_df[['Year']])\n",
    "\n",
    "# Persist scalers for reuse\n",
    "joblib.dump(turnover_scaler, ARTIFACTS_DIR / 'turnover_scaler.joblib')\n",
    "joblib.dump(year_scaler, ARTIFACTS_DIR / 'year_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1759231800924,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "zV51TXwWXZsB",
    "outputId": "41d07aaa-0e31-45cd-be58-4a031da880d5"
   },
   "outputs": [],
   "source": [
    "# Tabular DNN inputs\n",
    "\n",
    "target_col = 'Turnover'\n",
    "dnn_categorical_cols = ['State', 'Industry', 'Series ID']\n",
    "dnn_numeric_cols = ['Year', 'MonthNum', 'Quarter', 'MonthSin', 'MonthCos']\n",
    "dnn_feature_cols = dnn_categorical_cols + dnn_numeric_cols\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False), dnn_categorical_cols),\n",
    "        ('numeric', StandardScaler(), dnn_numeric_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_dnn = preprocessor.fit_transform(train_df[dnn_feature_cols]).astype(np.float32)\n",
    "X_val_dnn = preprocessor.transform(val_df[dnn_feature_cols]).astype(np.float32)\n",
    "X_test_dnn = preprocessor.transform(test_df[dnn_feature_cols]).astype(np.float32)\n",
    "\n",
    "y_train = train_df[target_col].to_numpy(np.float32)\n",
    "y_val = val_df[target_col].to_numpy(np.float32)\n",
    "y_test = test_df[target_col].to_numpy(np.float32)\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, ARTIFACTS_DIR / 'dnn_preprocessor.joblib')\n",
    "\n",
    "print('DNN input dim:', X_train_dnn.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1759231803541,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "NmtDFQZrXZsC",
    "outputId": "ec99fa31-4e36-4b54-cc10-c9eae95c4a0f"
   },
   "outputs": [],
   "source": [
    "# Sequence data builders (reuse from Task 1)\n",
    "WINDOW_SIZE_DEFAULT = 12\n",
    "HORIZON = 1\n",
    "seq_feature_cols = ['TurnoverScaled', 'MonthSin', 'MonthCos', 'YearScaled', 'SeriesIndexNorm']\n",
    "seq_target_col = 'TurnoverScaled'\n",
    "\n",
    "def build_sequence_arrays(df: pd.DataFrame, feature_cols: list[str], target_col: str, window: int, horizon: int):\n",
    "    sequences, targets, actuals = [], [], []\n",
    "    for _, group in df.groupby('Series ID'):\n",
    "        group = group.sort_values('Month')\n",
    "        feat = group[feature_cols].to_numpy(np.float32)\n",
    "        targ = group[target_col].to_numpy(np.float32)\n",
    "        actual = group['Turnover'].to_numpy(np.float32)\n",
    "        n = len(group) - window - horizon + 1\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        for i in range(n):\n",
    "            sequences.append(feat[i:i+window])\n",
    "            targets.append(targ[i+window:i+window+horizon])\n",
    "            actuals.append(actual[i+window:i+window+horizon])\n",
    "    if not sequences:\n",
    "        return (\n",
    "            np.empty((0, window, len(feature_cols)), dtype=np.float32),\n",
    "            np.empty((0, horizon), dtype=np.float32),\n",
    "            np.empty((0, horizon), dtype=np.float32),\n",
    "        )\n",
    "    return np.stack(sequences), np.stack(targets), np.stack(actuals)\n",
    "\n",
    "X_seq_train, y_seq_train, _ = build_sequence_arrays(train_df, seq_feature_cols, seq_target_col, WINDOW_SIZE_DEFAULT, HORIZON)\n",
    "X_seq_val, y_seq_val, _ = build_sequence_arrays(val_df, seq_feature_cols, seq_target_col, WINDOW_SIZE_DEFAULT, HORIZON)\n",
    "X_seq_test, y_seq_test_scaled, y_seq_test_actual = build_sequence_arrays(test_df, seq_feature_cols, seq_target_col, WINDOW_SIZE_DEFAULT, HORIZON)\n",
    "\n",
    "print('Seq shapes -> train:', X_seq_train.shape, 'val:', X_seq_val.shape, 'test:', X_seq_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1759231810189,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "ZWHLcmT3XZsC"
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "def make_datasets(X_train, y_train, X_val, y_val, batch_size):\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .shuffle(buffer_size=len(X_train), seed=RANDOM_SEED, reshuffle_each_iteration=True)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(obj, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1759231813431,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "LaW3aEjxXZsC"
   },
   "outputs": [],
   "source": [
    "# Tuning search spaces\n",
    "\n",
    "MAX_EPOCHS = 80\n",
    "DNN_TRIALS = 30\n",
    "GRU_TRIALS = 30\n",
    "\n",
    "\n",
    "def build_dnn_model_hp(input_dim: int, hp: 'kt.HyperParameters') -> keras.Model:\n",
    "    inputs = keras.Input(shape=(input_dim,), name='tabular_features')\n",
    "    x = inputs\n",
    "\n",
    "    num_layers = hp.Int('dnn_num_layers', 2, 4)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'dnn_units_{i}', min_value=64, max_value=512, step=64)\n",
    "        x = keras.layers.Dense(units, activation=hp.Choice('dnn_activation', ['relu', 'gelu']))(x)\n",
    "        if hp.Boolean('dnn_batchnorm', default=True):\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "        dropout = hp.Float('dnn_dropout', 0.0, 0.5, step=0.1)\n",
    "        if dropout > 0:\n",
    "            x = keras.layers.Dropout(dropout)(x)\n",
    "    outputs = keras.layers.Dense(1, name='turnover')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DNN_Tuned')\n",
    "    lr = hp.Float('dnn_lr', 1e-4, 5e-3, sampling='log')\n",
    "    wd = hp.Float('dnn_weight_decay', 1e-6, 1e-3, sampling='log')\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=[\n",
    "            keras.metrics.MeanSquaredError(name='mse'),\n",
    "            keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "            keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            keras.metrics.MeanAbsolutePercentageError(name='mape'),\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_gru_model_hp(window: int, feature_dim: int, horizon: int, hp: 'kt.HyperParameters') -> keras.Model:\n",
    "    inputs = keras.Input(shape=(window, feature_dim), name='sequence_features')\n",
    "    x = inputs\n",
    "\n",
    "    num_layers = hp.Int('gru_num_layers', 1, 2)\n",
    "    units1 = hp.Int('gru_units1', 64, 256, step=64)\n",
    "    x = keras.layers.GRU(units1, return_sequences=(num_layers > 1), kernel_initializer='glorot_uniform')(x)\n",
    "    if hp.Boolean('gru_layernorm', default=True):\n",
    "        x = keras.layers.LayerNormalization()(x)\n",
    "    drop1 = hp.Float('gru_dropout1', 0.0, 0.5, step=0.1)\n",
    "    if drop1 > 0:\n",
    "        x = keras.layers.Dropout(drop1)(x)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        units2 = hp.Int('gru_units2', 64, 256, step=64)\n",
    "        x = keras.layers.GRU(units2, return_sequences=False, kernel_initializer='glorot_uniform')(x)\n",
    "        drop2 = hp.Float('gru_dropout2', 0.0, 0.5, step=0.1)\n",
    "        if drop2 > 0:\n",
    "            x = keras.layers.Dropout(drop2)(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(horizon, kernel_initializer='glorot_uniform', name='scaled_turnover')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='GRU_Tuned')\n",
    "    lr = hp.Float('gru_lr', 1e-4, 5e-3, sampling='log')\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=[\n",
    "            keras.metrics.MeanSquaredError(name='mse'),\n",
    "            keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "            keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            keras.metrics.MeanAbsolutePercentageError(name='mape'),\n",
    "        ],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3789237,
     "status": "ok",
     "timestamp": 1759235627085,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "mrJw38TgXZsD",
    "outputId": "690b8071-d851-44e6-8a1e-f44a545a503f"
   },
   "outputs": [],
   "source": [
    "# Run Keras Tuner for DNN (BayesianOptimization with explicit max_trials)\n",
    "\n",
    "if kt is None:\n",
    "    print('Skip Keras Tuner for DNN since not available.')\n",
    "else:\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        hypermodel=lambda hp: build_dnn_model_hp(X_train_dnn.shape[1], hp),\n",
    "        objective=kt.Objective('val_loss', direction='min'),\n",
    "        max_trials=DNN_TRIALS,\n",
    "        seed=RANDOM_SEED,\n",
    "        directory=str(ARTIFACTS_DIR / 'kt_dnn'),\n",
    "        project_name='dnn_bayes',\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    batch_size = 256\n",
    "    train_ds, val_ds = make_datasets(X_train_dnn, y_train, X_val_dnn, y_val, batch_size)\n",
    "\n",
    "    stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[stop, reduce],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    save_json(best_hps.values, ARTIFACTS_DIR / 'dnn_best_hyperparameters.json')\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    history = best_model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        callbacks=[stop, reduce],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    # Evaluate and save\n",
    "    dnn_eval = best_model.evaluate(tf.data.Dataset.from_tensor_slices((X_test_dnn, y_test)).batch(256), return_dict=True, verbose=0)\n",
    "    dnn_preds = best_model.predict(X_test_dnn, batch_size=256, verbose=0).squeeze()\n",
    "    dnn_metrics = compute_metrics(y_test, dnn_preds)\n",
    "    print('DNN tuned test metrics:', dnn_metrics)\n",
    "\n",
    "    best_model.save(MODELS_DIR / 'dnn_tuned.keras')\n",
    "    save_json({'val_history': history.history, 'test_metrics': dnn_metrics}, ARTIFACTS_DIR / 'dnn_tuned_metrics.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8214230,
     "status": "ok",
     "timestamp": 1759254369093,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "kwnO7ujeXZsD",
    "outputId": "84b7a3b3-7b3d-4d9f-ef18-b259c0b5a50e"
   },
   "outputs": [],
   "source": [
    "# Run Keras Tuner for GRU (faster settings: fewer trials/epochs, cached pipeline, no retrain)\n",
    "\n",
    "if kt is None:\n",
    "    print('Skip Keras Tuner for GRU since not available.')\n",
    "else:\n",
    "    # Reduce total search effort for speed\n",
    "    fast_trials = min(12, GRU_TRIALS)\n",
    "    fast_epochs = min(30, MAX_EPOCHS)\n",
    "\n",
    "    tuner_gru = kt.BayesianOptimization(\n",
    "        hypermodel=lambda hp: build_gru_model_hp(WINDOW_SIZE_DEFAULT, X_seq_train.shape[2], HORIZON, hp),\n",
    "        objective=kt.Objective('val_loss', direction='min'),\n",
    "        max_trials=fast_trials,\n",
    "        seed=RANDOM_SEED,\n",
    "        directory=str(ARTIFACTS_DIR / 'kt_gru'),\n",
    "        project_name='gru_bayes',\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    # Choose batch size based on device capability\n",
    "    try:\n",
    "        has_gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "    except Exception:\n",
    "        has_gpu = False\n",
    "    batch_size = 256 if has_gpu else 128\n",
    "\n",
    "    # Faster tf.data pipeline with cache + prefetch\n",
    "    def make_fast_seq_datasets(Xtr, ytr, Xva, yva, bs):\n",
    "        train = (tf.data.Dataset.from_tensor_slices((Xtr, ytr))\n",
    "                 .shuffle(min(len(Xtr), 10000), seed=RANDOM_SEED, reshuffle_each_iteration=True)\n",
    "                 .batch(bs)\n",
    "                 .cache()\n",
    "                 .prefetch(AUTOTUNE))\n",
    "        val = (tf.data.Dataset.from_tensor_slices((Xva, yva))\n",
    "               .batch(bs)\n",
    "               .cache()\n",
    "               .prefetch(AUTOTUNE))\n",
    "        return train, val\n",
    "\n",
    "    train_ds, val_ds = make_fast_seq_datasets(X_seq_train, y_seq_train, X_seq_val, y_seq_val, batch_size)\n",
    "\n",
    "    # Tighter early stopping to cut epochs earlier\n",
    "    stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
    "\n",
    "    tuner_gru.search(\n",
    "        train_ds,\n",
    "        epochs=fast_epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[stop, reduce],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    # Pick best hyperparameters\n",
    "    best_hps = tuner_gru.get_best_hyperparameters(num_trials=1)[0]\n",
    "    save_json(best_hps.values, ARTIFACTS_DIR / 'gru_best_hyperparameters.json')\n",
    "\n",
    "    # Reuse the trained best model from the search to avoid a full second fit\n",
    "    best_model = tuner_gru.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Evaluate and save\n",
    "    test_seq_ds = (tf.data.Dataset\n",
    "                   .from_tensor_slices((X_seq_test, y_seq_test_scaled))\n",
    "                   .batch(batch_size)\n",
    "                   .cache()\n",
    "                   .prefetch(AUTOTUNE))\n",
    "    gru_eval = best_model.evaluate(test_seq_ds, return_dict=True, verbose=0)\n",
    "    preds_scaled = best_model.predict(X_seq_test, batch_size=batch_size, verbose=0)\n",
    "    preds = turnover_scaler.inverse_transform(preds_scaled)\n",
    "    y_true = y_seq_test_actual.squeeze(axis=-1)\n",
    "    y_pred = preds.squeeze(axis=-1)\n",
    "    gru_metrics = compute_metrics(y_true, y_pred)\n",
    "    print('GRU tuned test metrics:', gru_metrics)\n",
    "\n",
    "    best_model.save(MODELS_DIR / 'gru_tuned.keras')\n",
    "    # No history when skipping retrain; log eval metrics instead\n",
    "    save_json({'val_best_hps': best_hps.values, 'test_metrics': gru_metrics, 'eval': gru_eval}, ARTIFACTS_DIR / 'gru_tuned_metrics.json')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
