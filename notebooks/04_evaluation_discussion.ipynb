{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csdyEgqlwU1a"
   },
   "source": [
    "# Assignment 2 - Task 3: Evaluation & Discussion\n",
    "\n",
    "This notebook loads the tuned models and artifacts, evaluates on the held-out test set, produces comparison tables and plots, and summarizes findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28914,
     "status": "ok",
     "timestamp": 1759287210085,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "fd1VgAudwU1c",
    "outputId": "88c5bb4a-4bee-41ae-bf38-c61ad64efff1"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/drive')\n",
    "\n",
    "# Resolve notebooks root inside Google Drive robustly\n",
    "\n",
    "def resolve_notebooks_root() -> Path:\n",
    "    env_root = os.environ.get('ODL_NOTEBOOKS_ROOT')\n",
    "    if env_root:\n",
    "        root = Path(env_root)\n",
    "        if (root / 'data/processed').exists():\n",
    "            return root\n",
    "    base = Path('/drive/My Drive')\n",
    "    candidates = [\n",
    "        base / 'Colab Notebooks' / 'notebooks',\n",
    "        base / 'ODL-Assignment-test' / 'notebooks',\n",
    "        base / 'ODL Assignment' / 'notebooks',\n",
    "        base / 'notebooks',\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if (cand / 'data/processed').exists():\n",
    "            return cand\n",
    "    # Brute-force search as last resort (can take a bit if Drive is large)\n",
    "    try:\n",
    "        for p in base.rglob('notebooks'):\n",
    "            if (p / 'data/processed').exists():\n",
    "                return p\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise FileNotFoundError(\n",
    "        'Could not locate notebooks root under Drive. Set env ODL_NOTEBOOKS_ROOT to the path that contains data/, artifacts/, models/ subfolders.'\n",
    "    )\n",
    "\n",
    "nb_root = resolve_notebooks_root()\n",
    "print('Using notebooks root:', nb_root)\n",
    "\n",
    "data_dir = nb_root / 'data' / 'processed'\n",
    "artifacts_dir = nb_root / 'artifacts'\n",
    "models_dir = nb_root / 'models' / 'tuned'\n",
    "\n",
    "train_df = pd.read_csv(data_dir / 'train.csv', parse_dates=['Month'])\n",
    "val_df = pd.read_csv(data_dir / 'val.csv', parse_dates=['Month'])\n",
    "test_df = pd.read_csv(data_dir / 'test.csv', parse_dates=['Month'])\n",
    "print('Test shape:', test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1759287219116,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "3pfttEe9wU1d"
   },
   "outputs": [],
   "source": [
    "# Recreate minimal features to align with tuning\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['MonthSin'] = np.sin(2 * np.pi * df['MonthNum'] / 12.0)\n",
    "    df['MonthCos'] = np.cos(2 * np.pi * df['MonthNum'] / 12.0)\n",
    "    return df\n",
    "\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    add_time_features(frame)\n",
    "\n",
    "# Map Series ID to index (same method as tuning)\n",
    "all_series_ids = pd.concat([\n",
    "    train_df['Series ID'], val_df['Series ID'], test_df['Series ID']\n",
    "], axis=0).unique()\n",
    "series_ids = sorted(all_series_ids)\n",
    "series_index = {sid: idx for idx, sid in enumerate(series_ids)}\n",
    "denominator = max(len(series_ids) - 1, 1)\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    frame['SeriesIndex'] = frame['Series ID'].map(series_index).astype('float32')\n",
    "    frame['SeriesIndexNorm'] = frame['SeriesIndex'] / denominator\n",
    "\n",
    "# Load scalers\n",
    "turnover_scaler = joblib.load(artifacts_dir / 'turnover_scaler.joblib')\n",
    "year_scaler = joblib.load(artifacts_dir / 'year_scaler.joblib')\n",
    "\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    frame['TurnoverScaled'] = turnover_scaler.transform(frame[['Turnover']])\n",
    "    frame['YearScaled'] = year_scaler.transform(frame[['Year']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2057,
     "status": "ok",
     "timestamp": 1759287238817,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "YTgZuK16wU1d",
    "outputId": "a9eca22b-a33a-40f9-bb77-1a5064426258"
   },
   "outputs": [],
   "source": [
    "# DNN evaluation\n",
    "preprocessor = joblib.load(artifacts_dir / 'dnn_preprocessor.joblib')\n",
    "\n",
    "X_test_dnn = preprocessor.transform(test_df[['State','Industry','Series ID','Year','MonthNum','Quarter','MonthSin','MonthCos']]).astype(np.float32)\n",
    "y_test = test_df['Turnover'].to_numpy(np.float32)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "# Try to load model; if absent, fall back to saved metrics JSON\n",
    "try:\n",
    "    dnn_model = keras.models.load_model(models_dir / 'dnn_tuned.keras')\n",
    "except Exception:\n",
    "    dnn_model = None\n",
    "\n",
    "if dnn_model is not None:\n",
    "    dnn_preds = dnn_model.predict(X_test_dnn, batch_size=256, verbose=0).squeeze()\n",
    "    dnn_metrics = compute_metrics(y_test, dnn_preds)\n",
    "    print('DNN tuned test metrics:', dnn_metrics)\n",
    "else:\n",
    "    dnn_metrics = None\n",
    "    dnn_metrics_path = artifacts_dir / 'dnn_tuned_metrics.json'\n",
    "    if dnn_metrics_path.exists():\n",
    "        try:\n",
    "            with open(dnn_metrics_path, 'r', encoding='utf-8') as f:\n",
    "                obj = json.load(f)\n",
    "            # Prefer explicit test_metrics if present\n",
    "            if isinstance(obj, dict) and 'test_metrics' in obj:\n",
    "                dnn_metrics = obj['test_metrics']\n",
    "                print('Loaded DNN metrics from JSON artifacts (no model loaded).')\n",
    "        except Exception as e:\n",
    "            print('Failed to read DNN metrics JSON:', e)\n",
    "    if dnn_metrics is None:\n",
    "        print('No tuned DNN model or metrics found. Skip.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3291,
     "status": "ok",
     "timestamp": 1759287272146,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "A3yF037qwU1e",
    "outputId": "7b2ded0f-649b-4a46-e129-d1727438bca2"
   },
   "outputs": [],
   "source": [
    "# GRU evaluation (sequence)\n",
    "WINDOW = 12\n",
    "HORIZON = 1\n",
    "seq_feature_cols = ['TurnoverScaled', 'MonthSin', 'MonthCos', 'YearScaled', 'SeriesIndexNorm']\n",
    "\n",
    "# Build test arrays\n",
    "\n",
    "def build_sequence_arrays(df: pd.DataFrame, feature_cols: list[str], target_col: str, window: int, horizon: int):\n",
    "    sequences, targets, actuals = [], [], []\n",
    "    for _, group in df.groupby('Series ID'):\n",
    "        group = group.sort_values('Month')\n",
    "        feat = group[feature_cols].to_numpy(np.float32)\n",
    "        targ = group[target_col].to_numpy(np.float32)\n",
    "        actual = group['Turnover'].to_numpy(np.float32)\n",
    "        n = len(group) - window - horizon + 1\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        for i in range(n):\n",
    "            sequences.append(feat[i:i+window])\n",
    "            targets.append(targ[i+window:i+window+horizon])\n",
    "            actuals.append(actual[i+window:i+window+horizon])\n",
    "    if not sequences:\n",
    "        return (\n",
    "            np.empty((0, window, len(feature_cols)), dtype=np.float32),\n",
    "            np.empty((0, horizon), dtype=np.float32),\n",
    "            np.empty((0, horizon), dtype=np.float32),\n",
    "        )\n",
    "    return np.stack(sequences), np.stack(targets), np.stack(actuals)\n",
    "\n",
    "X_seq_test, y_seq_test_scaled, y_seq_test_actual = build_sequence_arrays(test_df, seq_feature_cols, 'TurnoverScaled', WINDOW, HORIZON)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "try:\n",
    "    gru_model = keras.models.load_model(models_dir / 'gru_tuned.keras')\n",
    "except Exception:\n",
    "    gru_model = None\n",
    "\n",
    "if gru_model is not None and len(X_seq_test) > 0:\n",
    "    preds_scaled = gru_model.predict(X_seq_test, batch_size=128, verbose=0)\n",
    "    preds = turnover_scaler.inverse_transform(preds_scaled)\n",
    "    y_true = y_seq_test_actual.squeeze(axis=-1)\n",
    "    y_pred = preds.squeeze(axis=-1)\n",
    "\n",
    "    gru_metrics = compute_metrics(y_true, y_pred)\n",
    "    print('GRU tuned test metrics:', gru_metrics)\n",
    "else:\n",
    "    gru_metrics = None\n",
    "    gru_metrics_path = artifacts_dir / 'gru_tuned_metrics.json'\n",
    "    if gru_metrics_path.exists():\n",
    "        try:\n",
    "            with open(gru_metrics_path, 'r', encoding='utf-8') as f:\n",
    "                obj = json.load(f)\n",
    "            if isinstance(obj, dict) and 'test_metrics' in obj:\n",
    "                gru_metrics = obj['test_metrics']\n",
    "                print('Loaded GRU metrics from JSON artifacts (no model loaded).')\n",
    "        except Exception as e:\n",
    "            print('Failed to read GRU metrics JSON:', e)\n",
    "    if gru_metrics is None:\n",
    "        print('No tuned GRU model or metrics found. Skip.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1759287290461,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "yzno3kZcwU1e",
    "outputId": "b18eb189-233b-463d-cbe8-e9d2235f0bb3"
   },
   "outputs": [],
   "source": [
    "# Comparison table and basic plots\n",
    "rows = []\n",
    "if dnn_metrics is not None:\n",
    "    rows.append({'Model': 'DNN Tuned', **dnn_metrics})\n",
    "if gru_metrics is not None:\n",
    "    rows.append({'Model': 'GRU Tuned', **gru_metrics})\n",
    "\n",
    "if rows:\n",
    "    comparison_df = pd.DataFrame(rows).set_index('Model')\n",
    "    display(comparison_df)\n",
    "else:\n",
    "    print('No metrics available to compare. Ensure tuned models exist or metrics JSON files are present in', artifacts_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1759287297257,
     "user": {
      "displayName": "Chiew Cheng",
      "userId": "01365830263559312477"
     },
     "user_tz": -480
    },
    "id": "FUAVAvGSwU1e",
    "outputId": "defc0f78-8022-44ec-e4b8-31e7e0992347"
   },
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatter (if DNN present)\n",
    "if dnn_model is not None:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(y_test, dnn_preds, s=5, alpha=0.5)\n",
    "    plt.xlabel('Actual Turnover')\n",
    "    plt.ylabel('Predicted Turnover (DNN)')\n",
    "    plt.title('DNN: Actual vs Predicted')\n",
    "    lims = [min(y_test.min(), dnn_preds.min()), max(y_test.max(), dnn_preds.max())]\n",
    "    plt.plot(lims, lims, 'r--')\n",
    "    plt.show()\n",
    "\n",
    "# GRU residuals histogram\n",
    "if gru_model is not None and 'y_pred' in locals():\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(residuals, bins=40, alpha=0.8)\n",
    "    plt.title('GRU: Residuals on test')\n",
    "    plt.xlabel('Actual - Predicted')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
