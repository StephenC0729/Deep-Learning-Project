## Deep Learning Project — Retail Turnover Forecasting

This repository contains the code and notebooks for a deep learning project that forecasts **monthly retail turnover** at the intersection of **state** and **industry**. The goal is to build models that provide accurate short‑horizon forecasts to support planning and policy analysis.

The work uses the **Australian Retail Trade** dataset (`aus_retail`) and implements two complementary deep learning models:

- A **tabular Deep Neural Network (DNN)** baseline.
- A **GRU sequence model** operating on rolling time‑series windows.

---

## 1. Dataset

- **Source dataset**: `aus_retail` from the R `tsibbledata` package.
- **Original data provider**: Australian Bureau of Statistics (ABS), _Retail Trade, Australia_.
- **Variables**:
  - `State`, `Industry`, `Series ID` (unique state–industry identifier),
  - `Month` (monthly timestamp),
  - `Turnover` (millions of Australian dollars).
- **Period**: 1982‑04 to 2018‑12.

### 1.1 Data file in this project

- Working CSV (not versioned in Git by default): `notebooks/data/retail.csv`.
- Processed splits generated by the EDA notebook:
  - `notebooks/data/processed/train.csv`
  - `notebooks/data/processed/val.csv`
  - `notebooks/data/processed/test.csv`

Large data files are ignored via `.gitignore` to keep the repository lightweight. If these files are not present, you can regenerate them by running the notebooks in Section 3.

### 1.2 Re‑creating `retail.csv` from `tsibbledata` (R)

If you have R installed, you can export the dataset yourself:

```r
install.packages(c("tsibbledata", "tsibble", "readr"))
library(tsibbledata)
library(tsibble)
library(readr)

data("aus_retail")
aus_retail |>
  as_tibble() |>
  write_csv("notebooks/data/retail.csv")
```

### 1.3 Licensing note

The dataset is derived from **Australian Bureau of Statistics** data and distributed via the **`tsibbledata`** R package.  
Before redistributing the full dataset, ensure that you comply with the **ABS** and **`tsibbledata`** license terms (attribution and any reuse restrictions). This repository is structured so that you can:

- Work locally with `retail.csv` and processed splits, and
- Share the **code and notebooks** publicly without needing to commit the large data files.

---

## 2. Project structure

- `docs/`
  - `assignment1_dataset_algorithm_literature.md` — extended report‑style document covering dataset description, model choices, literature review, preprocessing, and notebook cell references.
- `notebooks/`
  - `01_eda_data_prep.ipynb` — exploratory data analysis (EDA), cleaning, feature engineering, and creation of processed train/val/test splits.
  - `02_model_building.ipynb` — model‑ready preprocessing and baseline model building (DNN and GRU) using the processed splits.
  - `03_model_tuning.ipynb` — hyperparameter tuning with Keras Tuner for both models; saves best hyperparameters, metrics, and tuned models.
  - `04_evaluation_discussion.ipynb` — evaluation, comparison of tuned models, visualizations, and discussion of results.
  - `artifacts/` — scalers, preprocessors, tuning logs, and best‑model artifacts (ignored by Git by default).
  - `data/` — raw and processed CSV data used by the notebooks (ignored by Git by default, except for small samples/README).
  - `models/` — saved tuned model weights (ignored by Git by default).
- `.gitignore` — configured to exclude large artifacts (models, scalers, Keras Tuner trials, and data files) from version control.
- `.gitattributes` — normalizes text file line endings across platforms.

---

## 3. Notebooks overview and execution order

The intended execution order is:

1. **`01_eda_data_prep.ipynb` — EDA & data preparation**

   - Load `notebooks/data/retail.csv`.
   - Perform basic profiling and cleaning:
     - Type parsing (`Month` to datetime, categorical casting of `State`, `Industry`, `Series ID`).
     - Removal of duplicates, rows with missing `Month`/`Turnover`, and non‑positive `Turnover` values.
   - Exploratory plots:
     - Distribution of `Turnover`, boxplots by `State`, overall time trend.
   - Global feature engineering:
     - Calendar features (`Year`, `MonthNum`, `Quarter`, `YearMonth`).
   - Chronological split into train/validation/test and saving processed CSVs:
     - `notebooks/data/processed/train.csv`
     - `notebooks/data/processed/val.csv`
     - `notebooks/data/processed/test.csv`

2. **`02_model_building.ipynb` — baseline models**

   - Load processed splits.
   - Shared preprocessing:
     - Cyclical time encodings (`MonthSin`, `MonthCos`).
     - Series index features (`SeriesIndex`, `SeriesIndexNorm`).
     - Scaling of `Turnover` and `Year`.
   - **DNN (tabular baseline)**:
     - `ColumnTransformer` for one‑hot encoding (`State`, `Industry`, `Series ID`) and scaling numeric calendar features.
     - Dense network with batch normalization and dropout; trained with Adam on MSE and evaluated via RMSE/MAE/MAPE/R².
   - **GRU sequence model**:
     - Construction of rolling windows (window = 12, horizon = 1) from scaled features.
     - GRU‑based architecture trained on sequences, with metrics computed after inverse scaling.

3. **`03_model_tuning.ipynb` — hyperparameter tuning**

   - Uses **Keras Tuner** (Bayesian search) to tune:
     - DNN architecture (depth, width, dropout, learning rate, etc.).
     - GRU architecture (number of units, layers, regularization, learning rate, etc.).
   - Saves:
     - Best hyperparameters (`notebooks/artifacts/*_best_hyperparameters.json`).
     - Tuned metrics (`notebooks/artifacts/*_tuned_metrics.json`).
     - Tuned models (`notebooks/models/tuned/*.keras`).

4. **`04_evaluation_discussion.ipynb` — evaluation & discussion**
   - Loads tuned models and evaluates them on the test set.
   - Compares DNN vs GRU on RMSE, MAE, MAPE, and R².
   - Produces plots and tables for inclusion in the written report.
   - Summarizes findings, limitations, and potential extensions.

The `docs/assignment1_dataset_algorithm_literature.md` file cross‑references specific cell numbers in these notebooks for grading and report screenshots.

---

## 4. Environment and setup

This project assumes **Python 3.9+** and a standard scientific Python stack. A typical environment will include:

- `numpy`, `pandas`, `scikit-learn`
- `matplotlib` / `seaborn`
- `tensorflow` / `keras`
- `keras-tuner`
- `jupyter` or VS Code with Jupyter support

### 4.1 Example setup with `venv`

```bash
python -m venv .venv
source .venv/Scripts/activate  # Windows PowerShell: .venv\Scripts\Activate.ps1
pip install -r requirements.txt  # if provided, or install the packages above manually
```

> Note: If `requirements.txt` is not present, install the listed packages manually using `pip install`.

---

## 5. Reproducibility notes

- **Data and artifacts**:
  - Large data files, models, and artifacts are excluded from Git via `.gitignore`.
  - To fully reproduce results, you must:
    1. Obtain/export `retail.csv` as described in Section 1.2.
    2. Run the notebooks in the order listed in Section 3 to regenerate processed data, scalers, and models.
- **Randomness**:
  - Notebooks use fixed random seeds where possible (e.g., 42) to improve reproducibility.
- **Hardware**:
  - Models can run on CPU but will train faster with a GPU‑enabled TensorFlow installation.

---

## 6. Citation

If you use this project or dataset, please cite:

- Australian Bureau of Statistics, _Retail Trade, Australia_ (data source).
- `tsibbledata` R package for the `aus_retail` dataset.
- Any additional references listed in `docs/assignment1_dataset_algorithm_literature.md`.
